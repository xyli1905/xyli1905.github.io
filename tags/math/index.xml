<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>math on Quanta</title>
    <link>https://xyli1905.github.io/tags/math/</link>
    <description>Recent content in math on Quanta</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 13 Aug 2019 23:26:24 -0700</lastBuildDate>
    
        <atom:link href="https://xyli1905.github.io/tags/math/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Response of Loss Functions to Label Noise (draft)</title>
      <link>https://xyli1905.github.io/2019/08/response-of-loss-function-to-label-noise/</link>
      <pubDate>Tue, 13 Aug 2019 23:26:24 -0700</pubDate>
      
      <guid>https://xyli1905.github.io/2019/08/response-of-loss-function-to-label-noise/</guid>
      <description>Distortion on dataset distribution by nosie labels Let $t$ and $t^*$ be the correct target label and the noisy label, respectively. Presently, we focus on binary classification and assume the label noise only depends on the correct label, i.e. the error rates are:
$$e _- = P(t^* = +1 | t= -1) \quad \text{and} \quad e _+ = P(t^*=-1|t=+1).$$
The noisy dataset defines the joint distribution $P(x, t^*)$, which relates to the clean distribution $P(x, t)$ through</description>
    </item>
    
    <item>
      <title>Bayes Classifier and MI Classifier With Noisy Labels</title>
      <link>https://xyli1905.github.io/2019/08/bayes-classifier-and-mi-classifier-with-noisy-labels/</link>
      <pubDate>Thu, 08 Aug 2019 18:50:27 -0700</pubDate>
      
      <guid>https://xyli1905.github.io/2019/08/bayes-classifier-and-mi-classifier-with-noisy-labels/</guid>
      <description>Definition and Relation This section is a partial review of Bao-Gang Hu&amp;rsquo;s paper.
Bayes Classifier without rejection and its Decision Rule Bayes Classifier makes decisions based on a (often) subjectively designed risk function $\mathfrak{R}$:
$$\mathfrak{R}(y_j | x) = \sum_i \lambda _{ij} P(x | t_i)P(t_i),$$
where $x \in \R^d$ is the input feature; $y_j$ and $t_i$ stand for the predicted class and the true class, respectively; $\lambda _{ij}$ is the cost when a true label $i$ is classified as $j$.</description>
    </item>
    
    <item>
      <title>Neural Tangent Kernel and NN Dynamics (under construction)</title>
      <link>https://xyli1905.github.io/2019/08/neural-tangent-kernel-and-nn-dynamics/</link>
      <pubDate>Thu, 01 Aug 2019 12:48:05 -0700</pubDate>
      
      <guid>https://xyli1905.github.io/2019/08/neural-tangent-kernel-and-nn-dynamics/</guid>
      <description>Discussion on Neural Tangent Kernel The Neural Tangent Kernel is introduced by Arthur Jacot et. al. to study the dynamics of a (S)GD-based learning model, say, the Neural Network. It is expected to enable one to &amp;ldquo;study the training of ANNs in the functional space $\mathcal{F}$, on which the cost $C$ is convex.&amp;rdquo;
In the following, we will review the derivations of Arthur Jacot et. al. and argue that by definition the Neural Tangent Kernel only captures the first order dynamics of the Neural Networks.</description>
    </item>
    
    <item>
      <title>Variational Form for H(x)</title>
      <link>https://xyli1905.github.io/2019/07/variational-form-for-hx/</link>
      <pubDate>Mon, 29 Jul 2019 10:24:47 -0700</pubDate>
      
      <guid>https://xyli1905.github.io/2019/07/variational-form-for-hx/</guid>
      <description>For continuum random variable $X$, the differential entropy id defined as
$$\int P(x)\log\dfrac{1}{P(x)}\,\text{d}x,\tag{1}$$
where $P(x)$ is the distribution for $X$.
function $-\log v$ is strictly convex, let $\phi$ be its conjugate dual function, then $-\log v = \underset{u\in\mathbb{R}}{\text{sup}}\left[uv - \phi(u)\right]$, $\phi(u) = -1 - \log(-u)$ for $u&amp;lt;0$ and $+\infty$ otherwise.
using above, we have
$$\begin{aligned} \int P(x)\log\frac{1}{P(x)}\,\text{d}x &amp;amp;= - \int P(x)\left(- \log\frac{1}{P(x)}\right)\,\text{d}x \\&amp;amp;= -\int P(x) \underset{f}{\text{sup}}\left[ f(x)\frac{1}{P(x)} - \phi(f) \right]\,\text{d}x \\&amp;amp;= -\underset{f}{\text{sup}}\left[ \int f(x)\,\text{d}x - \int \phi(f)P(x)\,\text{d}x\right] \\&amp;amp;= -\underset{f}{\text{sup}}\left[ \int f(x)\,\text{d}x - \mathbb{E}_{P}[\phi(f)]\right]\end{aligned}$$</description>
    </item>
    
  </channel>
</rss>
