<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>math on Quanta</title>
    <link>https://xyli1905.github.io/tags/math/</link>
    <description>Recent content in math on Quanta</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 01 Aug 2019 12:48:05 -0700</lastBuildDate>
    
        <atom:link href="https://xyli1905.github.io/tags/math/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Neural Tangent Kernel and NN Dynamics (under construction)</title>
      <link>https://xyli1905.github.io/2019/08/neural-tangent-kernel-and-nn-dynamics/</link>
      <pubDate>Thu, 01 Aug 2019 12:48:05 -0700</pubDate>
      
      <guid>https://xyli1905.github.io/2019/08/neural-tangent-kernel-and-nn-dynamics/</guid>
      <description>Discussion on Neural Tangent Kernel The Neural Tangent Kernel is introduced by Arthur Jacot et. al. to study the dynamics of a (S)GD-based learning model, say, the Neural Network. It is expected to enable one to &amp;ldquo;study the training of ANNs in the functional space $\mathcal{F}$, on which the cost $C$ is convex.&amp;rdquo;
In the following, we will review the derivations of Arthur Jacot et. al. and argue that by definition the Neural Tangent Kernel only captures the first order dynamics of the Neural Networks.</description>
    </item>
    
    <item>
      <title>Variational Form for H(x)</title>
      <link>https://xyli1905.github.io/2019/07/variational-form-for-hx/</link>
      <pubDate>Mon, 29 Jul 2019 10:24:47 -0700</pubDate>
      
      <guid>https://xyli1905.github.io/2019/07/variational-form-for-hx/</guid>
      <description>For continuum random variable $X$, the differential entropy id defined as
$$\int P(x)\log\dfrac{1}{P(x)}\,\text{d}x,\tag{1}$$
where $P(x)$ is the distribution for $X$.
function $-\log v$ is strictly convex, let $\phi$ be its conjugate dual function, then $-\log v = \underset{u\in\mathbb{R}}{\text{sup}}\left[uv - \phi(u)\right]$, $\phi(u) = -1 - \log(-u)$ for $u&amp;lt;0$ and $+\infty$ otherwise.
using above, we have
$$\begin{aligned} \int P(x)\log\frac{1}{P(x)}\,\text{d}x &amp;amp;= - \int P(x)\left(- \log\frac{1}{P(x)}\right)\,\text{d}x \\&amp;amp;= -\int P(x) \underset{f}{\text{sup}}\left[ f(x)\frac{1}{P(x)} - \phi(f) \right]\,\text{d}x \\&amp;amp;= -\underset{f}{\text{sup}}\left[ \int f(x)\,\text{d}x - \int \phi(f)P(x)\,\text{d}x\right] \\&amp;amp;= -\underset{f}{\text{sup}}\left[ \int f(x)\,\text{d}x - \mathbb{E}_{P}[\phi(f)]\right]\end{aligned}$$</description>
    </item>
    
  </channel>
</rss>
